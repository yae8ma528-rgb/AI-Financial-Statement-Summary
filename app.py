import streamlit as st
import streamlit.components.v1 as components # Javascriptæ³¨å…¥ç”¨
from google import genai  # â† ã“ã“ãŒå¤‰ã‚ã£ãŸï¼
from google.genai import types
from google.genai import errors # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç”¨
import tempfile
import os
from bs4 import BeautifulSoup
import time # ãƒªãƒˆãƒ©ã‚¤æ™‚ã®waitç”¨
import prompts # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«
import uuid # ãƒªã‚»ãƒƒãƒˆæ™‚ã®ã‚­ãƒ¼ç”Ÿæˆç”¨

# APIã‚­ãƒ¼è¨­å®šï¼ˆStreamlitã®secretsã‹ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
# api_key = os.environ.get("GEMINI_API_KEY") 

st.set_page_config(
    page_title="æ±ºç®—æ›¸ã¾ã¨ã‚Bot",
    page_icon="ğŸ¤–",
    layout="centered",
)

# ãƒ–ãƒ©ã‚¦ã‚¶ã«æ—¥æœ¬èªã‚µã‚¤ãƒˆã¨ã—ã¦èªè­˜ã•ã›ã‚‹ãŸã‚ã®Javascriptãƒãƒƒã‚¯
# st.markdownã§ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå®Ÿè¡Œã•ã‚Œãªã„å ´åˆãŒã‚ã‚‹ãŸã‚componentsã‚’ä½¿ç”¨
components.html("""
    <script>
        window.parent.document.getElementsByTagName('html')[0].lang = 'ja';
    </script>
""", height=0) 

st.title("æ±ºç®—æ›¸ã¾ã¨ã‚Bot v0.2.2Î²")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒªã‚»ãƒƒãƒˆæ©Ÿèƒ½
with st.sidebar:
    st.header("ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
    if st.button("åˆ†æã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state.confirm_reset = True

    if st.session_state.get("confirm_reset"):
        st.warning("æœ¬å½“ã«ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã‹ï¼Ÿ\nä¼šè©±å±¥æ­´ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("å‰Šé™¤"):
                # Geminiä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                if st.session_state.get("uploaded_gemini_file_name"):
                    try:
                        client = get_gemini_client()
                        client.files.delete(name=st.session_state.uploaded_gemini_file_name)
                        st.sidebar.success("ã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                    except Exception as e:
                        st.sidebar.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
                for key in list(st.session_state.keys()):
                    del st.session_state[key]
                
                # æ–°ã—ã„uploader_keyã‚’è¨­å®šã—ã¦ãƒªã‚»ãƒƒãƒˆ
                st.session_state.uploader_key = str(uuid.uuid4())
                st.rerun()
        with col2:
            if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«"):
                st.session_state.confirm_reset = False
                st.rerun()

if "chat_session" not in st.session_state:
    st.session_state.chat_session = None
if "summary_done" not in st.session_state:
    st.session_state.summary_done = False
if "messages" not in st.session_state:
    st.session_state.messages = []
if "uploaded_gemini_file_name" not in st.session_state:
    st.session_state.uploaded_gemini_file_name = None
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = str(uuid.uuid4())

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
@st.cache_resource
def get_gemini_client():
    return genai.Client(api_key=st.secrets["GEMINI_API_KEY"]) # ã‚­ãƒ¼ã¯é©åˆ‡ã«è¨­å®š

client = get_gemini_client()

uploaded_file = st.file_uploader("æ±ºç®—æ›¸(PDF or HTML)ã‚’æ·»ä»˜ã—ã¦ãã ã•ã„ã€‚", type=["pdf", "htm", "html"], key=st.session_state.uploader_key)

if uploaded_file and not st.session_state.summary_done:
    with st.spinner("AIãŒè§£æä¸­ã§ã™..."):
        
        content_to_send = "" # æ–‡å­—åˆ—ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå…¥ã‚‹
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆã“ã“ã¯ãƒ­ã‚¸ãƒƒã‚¯åŒã˜ï¼‰
        file_ext = os.path.splitext(uploaded_file.name)[1].lower()

        if file_ext == ".pdf":
            tmp_path = None
            try:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_path = tmp_file.name
                
                # ã€å¤‰æ›´ç‚¹1ã€‘ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                # configå¼•æ•°ã§display_nameã‚’æŒ‡å®šã™ã‚‹
                uploaded_gemini_file = client.files.upload(
                    file=tmp_path, 
                    config={'display_name': 'Earnings Report PDF'}
                )
                content_to_send = uploaded_gemini_file
                # ãƒªã‚»ãƒƒãƒˆç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¿å­˜
                st.session_state.uploaded_gemini_file_name = uploaded_gemini_file.name
            finally:
                # ç¢ºå®Ÿã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹
                if tmp_path and os.path.exists(tmp_path):
                    os.remove(tmp_path) 

        elif file_ext in [".htm", ".html"]:
            # HTMLå‡¦ç†ï¼ˆBeautifulSoupéƒ¨åˆ†ã¯ãã®ã¾ã¾ï¼‰
            try:
                html_content = uploaded_file.getvalue().decode("utf-8")
            except UnicodeDecodeError:
                html_content = uploaded_file.getvalue().decode("cp932")
            
            soup = BeautifulSoup(html_content, 'html.parser')
            for script_or_style in soup(["script", "style"]):
                script_or_style.decompose()
            text_data = soup.get_text(separator="\n") 
            lines = [line.strip() for line in text_data.splitlines() if line.strip()]
            clean_text = "\n".join(lines)
            
            content_to_send = clean_text

        # ã€å¤‰æ›´ç‚¹2ã€‘ãƒãƒ£ãƒƒãƒˆé–‹å§‹ & ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°/Fallbackå®Ÿè£…
        # è¨­å®šã‚’å…±é€šåŒ–
        # system_instruction ã¯ prompts.py ã‹ã‚‰èª­ã¿è¾¼ã‚€
        generation_config = types.GenerateContentConfig(
            system_instruction=prompts.SYSTEM_INSTRUCTION,
            temperature=0.2
        )

        # ãƒ¢ãƒ‡ãƒ«å®šç¾©
        primary_model = "gemini-2.5-flash"
        fallback_model = "gemini-2.5-flash-lite"
        
        # ã€å¤‰æ›´ç‚¹3ã€‘æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ã¨Fallbackãƒ«ãƒ¼ãƒ—
        # PDF(File object)ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æ··ãœã¦é€ã‚‹å ´åˆ
        prompt_text = prompts.PROMPT_FINANCIAL_SUMMARY
        
        models_to_try = [primary_model, fallback_model]
        active_chat = None
        response_text = ""

        for model_name in models_to_try:
            try:
                # Chatã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
                chat = client.chats.create(
                    model=model_name,
                    config=generation_config,
                    history=[]
                )
                
                # é€ä¿¡
                response = chat.send_message([content_to_send, prompt_text])
                
                # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                active_chat = chat
                response_text = response.text
                st.session_state.current_model = model_name # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
                break

            except errors.ClientError as e:
                # 429 Resource Exhausted (Rate Limit) ã®å ´åˆ
                if e.code == 429 or "429" in str(e):
                    st.warning(f"ãƒ¢ãƒ‡ãƒ« {model_name} ãŒæ··é›‘ã—ã¦ã„ã¾ã™(429)ã€‚æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
                    time.sleep(1) # ä¸€å‘¼å¸ç½®ã
                    continue
                else:
                    st.error(f"APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    break
            except Exception as e:
                st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                break
        
        if active_chat:
            st.session_state.chat_session = active_chat
            st.session_state.messages.append({"role": "assistant", "content": response_text})
            st.session_state.summary_done = True
        else:
            st.error("ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")

# è¡¨ç¤ºéƒ¨åˆ†ã¯å¤‰æ›´ãªã—
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ä»–ã«èããŸã„ã“ã¨ã¯ï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    if st.session_state.chat_session:
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                # æ—¢å­˜ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ãƒˆãƒ©ã‚¤
                response = st.session_state.chat_session.send_message(prompt)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
                with st.chat_message("assistant"):
                    st.markdown(response.text)

            except errors.ClientError as e:
                # 429ç™ºç”Ÿæ™‚ã€ã‹ã¤ä»Šã®ãƒ¢ãƒ‡ãƒ«ãŒprimaryãªã‚‰fallbackã¸ç§»è¡Œ
                is_rate_limit = e.code == 429 or "429" in str(e)
                current_model = st.session_state.get("current_model", "gemini-2.5-flash")
                fallback_model = "gemini-2.5-flash-lite"
                
                if is_rate_limit and current_model != fallback_model:
                    st.warning(f"ãƒ¢ãƒ‡ãƒ« {current_model} ãŒæ··é›‘ã—ã¦ã„ã¾ã™ã€‚{fallback_model} ã«åˆ‡ã‚Šæ›¿ãˆã¦å†è©¦è¡Œã—ã¾ã™...")
                    
                    try:
                        # å±¥æ­´ã‚’å¼•ãç¶™ã„ã§æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
                        # æ³¨æ„: å¤ã„historyã«ã¯å‰å›ã®ã‚„ã‚Šå–ã‚ŠãŒå«ã¾ã‚Œã¦ã„ã‚‹
                        old_history = st.session_state.chat_session.history
                        
                        # System instructionå†å®šç¾©ï¼ˆprompts.pyã‹ã‚‰å‚ç…§ï¼‰
                        
                        new_chat = client.chats.create(
                            model=fallback_model,
                            config=types.GenerateContentConfig(
                                system_instruction=prompts.SYSTEM_INSTRUCTION,
                                temperature=0.2
                            ),
                            history=old_history
                        )
                        
                        # å†é€ä¿¡
                        response = new_chat.send_message(prompt)
                        
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ›´æ–°
                        st.session_state.chat_session = new_chat
                        st.session_state.current_model = fallback_model
                        
                        st.session_state.messages.append({"role": "assistant", "content": response.text})
                        with st.chat_message("assistant"):
                            st.markdown(response.text)
                            
                    except Exception as retry_e:
                        st.error(f"å†è©¦è¡Œã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {retry_e}")
                else:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

            except Exception as e:
                st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
